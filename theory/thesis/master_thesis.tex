\documentclass[english]{article}
\usepackage[utf8]{inputenc}
%\usepackage{babel}
\usepackage[backend=bibtex]{biblatex}
\usepackage{hyperref}
\usepackage{mathtools}


\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\bibliography{master.bib}

% Add mathematical foo
\newcommand{\EI}{\operatorname{EI}}
\newcommand{\normal}{\mathcal{N}}

\begin{document}  

\title{Bayesian Hyperparameter-Optimization in Grammer-Based Search Spaces}
\author{Moritz Meier}
\maketitle
\tableofcontents
\newpage

\section{Introduction}
\paragraph{Motivation}

\section{Global Black-Box Optimization}
 - Local Optimization
 - Global Optimization
 - Criteria for optimization

\subsection{Bandit Problem}
Mathematical formulation of stochastic black-box function optimization.

\section{Hyperparameter Optimization}

\subsection{Machine Learning as Optimization}
 - Types and Error Functions
 - Model Selection

\begin{quote}
Machine learning algorithms, however, have certain characteristics that distinguish them from other
black-box optimization problems.  First, each function evaluation can require a variable amount of
time:  training a small neural network with 10 hidden units will take less time than a bigger net-
work with 1000 hidden units.  Even without considering duration, the advent of cloud computing
makes it possible to quantify economically the cost of requiring large-memory machines for learn-
ing, changing the actual cost in dollars of an experiment with a different number of hidden units.
Second, machine learning experiments are often run in parallel, on multiple cores or machines.  In
both situations, the standard sequential approach of GP optimization can be suboptimal.
\end{quote}
Copied from: \cite{snoek_practical_2012}

\subsection{Algorithm Types}
 - Manual Search
 - Random Search
 - Grid Search
\subsection{Search Space}
\subsubsection{Structure}
 - Categrorical
 - Discrete
 - Continunous
\paragraph{Assumptions}
\subsection{Search Space Construction}
\subsubsection{Search Space Hierachy}


\begin{tabular}{ l | c | c }
Language Feature & Search Space & Parameter Structure \\
\hline
atomic parameters   & one-dim          & single value \\
join                & tree             &              \\
product / operators & multi-dim        & tree         \\
recursion           & infinite tree    &              \\
expression equality & DAG              &              \\
value identity      & ?                & DAG          \\
recursive values    & ?                & ? 
\end{tabular}

\subsection{Demands to a good HPO-Algorithm}
A non-exaustive list of general criteria for a good hyperparameter optimization algorithm.

\section{Gaussian Processes Regression}
\subsection{Kernel Combination}
Kernel combination properties. Maybe: Why do they work!

\section{Bayesian Optimization}
What distinguished HPO from other non-HPO.

\subsection{Aquisition Functions}

\subsubsection{Probability of Improvement}
\subsubsection{Expected Information}

Expected Information (EI) is the most widely used aquistion function.

$$ \EI(y|x) \coloneqq \int_{-\infty}^{\infty} \max(0, y^*-y)p(y|x)dy $$

For the one-dimensional case a simple closes form formula can be derivated:

$$ \EI(y|x) = \int_{-\infty}^{y^*}(y^*-y)p(y|x)dy$$

$$ = \int_{-\infty}^{y^*}(y^*-y)\normal(y; \mu, \sigma)dy =
\int_{-\infty}^{y^*}(y^*-y)\normal(y-\mu; 0, \sigma)dy$$

$$ = \int_{-\infty}^{y^*}(y^*- t - \mu)\normal(t; 0, \sigma)dt $$ 

$$ = (y^*-\mu)\int_{-\infty}^{y^*}\normal(t; 0, \sigma)dt - \int_{-\infty}^{y^*}t\normal(t; 0, \sigma)dt$$

$$ = (y^*-\mu)\Phi(y^*/\sigma) - \phi(y^*/\sigma) $$

Where $\phi$ and $\Phi$ refere to the PDF and CDF of the standart normal distribution.

\subsubsection{Entropy search}
\subsubsection{Upper confidence bound}

\subsection{SMBO}
Sequential model-based optimization is a very general algorithmic pattern for global optimization with aquistion functions.
\subsection{SMAC}


\subsection{The Tree-Parzen Algorithm}
The Tree-Parzen Algorithm \cite{bergstra_algorithms_2011} is a Bayesian-Optimization algorithm for tree-structured parameter spaces.

\subsection{Tree and Graph Kernels/Metrics}


\section{The Tree Farm Algorithm}
Presenting my algorithm for Bayesian optimization of Grammar-Based search spaces.
\subsection{Extention to DAG-structured search spaces}
Identifying equivalent function trees with computer algebra systems.
\subsection{Parallelization}
Simple trick to extend algorithm to parrallel execution.
\subsection{Integration with Manual Search}
Manual guidance before and during the optimization process.
\subsubsection{Prior Information}
\subsubsection{Search Hints}
\subsection{Objective Function Approximation}


\section{Implementation}
\subsection{Search Space Construction DSL}
Calls are 
\subsection{Algorithm Templates}


\section{Experiments}



\subsection{Gaussian Process Regression}
\subsubsection{Kernel Combination Optimization}
\subsubsection{Kernel Parameter and Kernel Combination Optimization}
\subsection{Elastic Net Linear Regression}
\subsection{Feat-Forward Networks}

\section{Conclusion}

\section{Outlook}
\paragraph{Connection to Functional Programming}
\paragraph{Connection to Constraint Satisfaction Problems}
\paragraph{Connection to Probabilistic Programming}
\paragraph{Connection to Reinforcement Learning}
\paragraph{Integration with Workflow-Managers}


\printbibliography

\end{document}  
