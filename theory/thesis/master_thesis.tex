\documentclass[english]{article}
\usepackage{amsfonts}
\usepackage{dot2texi}
\usepackage[utf8]{inputenc}
%\usepackage{babel}
\usepackage[backend=bibtex]{biblatex}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage[printonlyused]{acronym}
\usepackage{minted}
\usepackage{amsthm}
%\usepackage{amsmath}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage[linesnumbered]{algorithm2e}
\usepackage{amssymb}

\usetikzlibrary{shapes,arrows}

\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\bibliography{library.bib}

% Add mathematical foo
\newcommand{\EI}{\operatorname{EI}}
\newcommand{\normal}{\mathcal{N}}

\begin{document}

\title{Bayesian Hyperparameter-Optimization in Grammer-Based Search Spaces}
\author{Moritz Meier}
\maketitle
\tableofcontents
\newpage

\section*{List of Acronyms}
  \begin{acronym}
  \acro{AI}{artificial intelligence}
  \acro{BO}{Bayesian Optimization}
  \acro{BOA}{Bayesian optimization algorithm}
  \acro{BGO}{Bayesian global optimization}
  \acro{DAG}{directed acyclic graph}
  \acro{EGO}{efficient global optimization}
  \acro{GO}{global optimization}
  \acro{HPO}{hyperparameter optimization}
  \acro{IAF}{information acquisition function}
  \acro{ML}{machine learning}
  \acro{SMAC}{sequential model-based algorithm configuration}
  \acro{SMBO}{sequential model-based optimization}
\end{acronym}

\section{Introduction}

Modern society heavly relies on automated information processing

The increading amount of digital data that arises in modern society poses a difficult challange for established information technology. Developers are forced to adapt  ...

During its history the field of software engeneering was subject to a continual change. New affordances to computer systems forced developers to consistanty adopt concepts from academic fields into their own skill sets. The raising complexity in software projects gave birth high level programming languages that are easier to grasp for the human mind, which makes code generation and maintainance tractable for complex applications.
In recent years, the increading amount of digital data, poses a untractable challange for conventional desing approaches. As a consequence techniques from  areas like machine learning are inceasingly applied.

Due to the large amount of digital information collected in modern society the need for automated data processing and analysis tools rises in many areas of the industry. The

This work proposes a formalism to state optimization problems by describing the structure of a wide range of search spaces together with an algorithm adopted from the field of hyperparamter-optimization as an approach to high level machine learning.



The mathematical branch of Optimization is mathematical programming, where a mathematical program referes to an optimization problem with additional equality and inequality constraints.

Metaheuristics
Programm Configuration
Learning \& Optimizaion.


\section{Global Black-Box Optimization}

\subsection{Distinctions}

\subsubsection*{gradient based vs. derivative free}
also

\subsubsection*{Local vs. Global}

\subsubsection*{Heuristic strategies vs. Exect methods}
aka. stochastic vs deterministic

\subsubsection*{model-based vs. instance-based}
More sophisticated \ac{GO} algorithms can be devided into
a.k.a. - model-free or direct

\subsubsection*{multi vs. single instance}

\subsubsection*{exploration-exploitation tradeoff}

\subsubsection*{interactive}

\subsubsection*{predictive vs. explanatory model}

\subsubsection*{parallelizabel}

\subsubsection*{stochastic objective function}

\subsubsection*{single vs multiobjective}

\subsubsection*{scalability}

\subsubsection*{approximations}
 - early stopping
 - data subset

 \subsection{computational costs}
 - space \& time
 - intelligent selection vs massive sampling tradeoff


The use of stochastic processe in \ac{GO} for surrogate fitting is called \ac{BO}. This approach was first explored by Harold Kushner in 1964 \cite{kushner_new_1964}. In \cite{jones_efficient_1998} the surrogate is also called \textit{figure of merrit}.


\subsection{Active Learning}


\subsection{Bandit Problem}


\section{Hyperparameter Optimization}
In Bayesain statistics the term \textit{hyperparameter} referes to the a parameter of the prior distribution. In \textit{hierarchical models} the priors distribution over the hyperparamters are called \textit{hyperpriors} respectively \cite[p.408]{bishop_neural_1995}. In \acf{ML} a hyperparameter is any parameter that needs to be assigned before the training of other parameters can begin.
\acf{HPO} Other terms for \ac{HPO} are hyperarameter \textit{tuning} or \textit{search}. Also \textit{model selection} means essentially the same thing, the focus is however more on the selection criterions.
The ususal \ac{HPO} procedure ...
Key characteristics of a \ac{HPO} problems are:
\paragraph{Long evaluation times}
\paragraph{Little knowledge} of the objective function
\paragraph{varying complexity} of search space ranging from simple homogenic cartesian search spaces to heterogenic graph shaped spaces.

\subsection{Machine Learning as Optimization}



Most \acf{ML} problems are expressable as optimization problems \cite{bennett_interplay_2006}.

\paragraph{Types and Error Functions}


\paragraph{Model Selection}



\begin{quote}
Machine learning algorithms, however, have certain characteristics that distinguish them from other black-box optimization problems.  First, each function evaluation can require a variable amount of time:  training a small neural network with 10 hidden units will take less time than a bigger net-work with 1000 hidden units.  Even without considering duration, the advent of cloud computing makes it possible to quantify economically the cost of requiring large-memory machines for learning, changing the actual cost in dollars of an experiment with a different number of hidden units.
\end{quote}
Second, machine learning experiment EGO algorithm
 - Categrorical
 - Discrete
 - Continunous


\paragraph{Assumptions}
...

\subsection{Structure and Construction of Search Spaces}
The majority of \ac{HPO} algorithms are designed for domains of the form $\mathbb{R}^n$.
The basic idea underlying the construction of parameter spaces is that variables do not need to possess a unique value assingment, but can rather have a set of or possible values. Instead of saying $x = 1$ one can say that $x$ equals ether one or two. Any other operations that Finally this set of computation graphes can be applied to a performance measure, expressing an optimization problem with instances of the computation graph set as possible solutions.

A search space is not a set but induces a unique set. The set of all instances that can be produced by a search space description $G$ is called the \textit{extension} of $G$ and expressed with a following apostrophe $G'$. While a unique extension can be assigned to every search space, an extension can be represented by multiple search spaces.

\subsubsection{Primitive Spaces}
The smallest unit of a search spaces is a primitive space. It is a one dimensional collection of possible instances of a parameter. Every search space is ether a primitive space of a combination of primitive spaces.

\paragraph{Categorical}
Categoriacal parameters have a finite unsorted domain of numerical or non-numerical values. This type is used for nominal scaled parameters like labels in a classification problem, basis-functions in linear regression or different activation functions in feedforward neural networks.

\paragraph{Continuous}
Continues parametes are bounded or unbounded intervals over the real numbers. This type would, for example, be used for a regularization parameter or the discount factor in Q-learning.

\paragraph{Discrete}
Values of a discrete parameters are numerical. The cardinality of a discrete parameter is finite or countable (if defined over an unbounded interval).
Other than categorical parameters, but similar to continous parameters, discrete parameters have a distance over their elements. If two elements are close, than the value of the objective fucntion is also assumed to be closed. This type is, for example, used as the number of layers in a neural network.

\subsubsection{Combinations}
Combinations take search spaces and return a combined search space. A combined search space is represented by an application node which posseses an operation and a domain field. Combined search spaces can be combined again in order to create strcutured search spaces. The operation field is assiged with combinator itself, while the domain is assigned with a parameter family.
In programming languages, when calling a function, the parameters are usually passed-by-name, passed-by-value or both. Mathematically this can be expressed by an indexed family. The passed-by-value arguemts are indexed by integers from $0$ to $n$ and the passde-by-name argumets are indexed by strings. A parameter family with two positional arguments $A,B$ and a key keyword argument $C$ with index $\gamma$ is written as:
$$\{A_0, B_1, C_\gamma\}$$
The cartesian product of two parameter families is defined as follows. If all indices are integers the product is a usual cartesian product. The length of the first family is added to all indices of the second tuples.
$$\{A_0, B_1\} \times \{C_0, D_1\} = \{A_0, B_1, C_2, D_3\}$$
Non-integer indices are simply copied to the resulting family.
$$\{A_0,\ B_\beta\} \times \{C_0,\ D_\delta\} = \{A_0,\ C_1, B_\beta,\ D_\delta\}$$
The prodcut is undefined if two non-positional parameters share the same key.
Items of a parameter family can be renamed. In resemblence to relational algebra the greek letter $\rho$ is used to refer to the rename.
$$\rho_{1/a}((A_{0}, B_{1})) = (A_{0}, B_{a}) = (A, a:B)$$
Elements can by selected from parameter family be the projection $\pi$.
$$\pi_1(\{A_0, B_1, C_\gamma\}) = B$$

\paragraph{Joining}
Two search spaces $A$ and $B$ can be joined to form a new search space, which contains all instances of $A$ and $B$. In recemblence to the Backus–Naur form, the join combination can also be expressed through a pipe symbol.
$$ \operatorname{join}(A, B) = A\ |\ B $$
The join is related to the set union, in that the extension of the join of two search spaces is the union of the extensions of these search spaces: $(A\ |\ B)' = A' \cup B'$. The only difference is that a join remembers the spaces it was constructed from.

\paragraph{Product}
The product lines up multiple search spaces to form a sequence of search spaces. The extension of a product of two spaces is the cartesian product of the extension of these two spaces: $\operatorname{prod}(A,B)' = A' \times B'$. In constrast to the cartesian product, the product of search spaces is associative.  In this way no nested structures can be constucted with the product alone.

\paragraph{Function Application}
With the apply combination the application of an abitrary function to the elements of a search space can be expressed.
$$ f(A \cup B) = f(A)\ |\ f(B) $$
Spaces can also be applied to other spaces.
$$(f\ |\ g)(A) = f(A)\ |\ g(A)$$

With function application it becomes possible to express \textit{dependendencies} between parameters.

$$A = f(B) \cup g(C)$$

If $B \neq C$ then the function $f$ and $g$ have different parameter domains. The choice of the parameter is dependent on previous choice of the function.  The structure of the search space is not a vector any more but becomes a tree.

\paragraph{Recursion}
With recursion it is possibel to express grammars.

$$ A = (\ \mathbb{N}\ \cup\ f(\mathbb{N}, A)\ ) $$

The extension of an expression is defined as the  :

$$ A = \{\mathbb{N},\ f(\mathbb{N}, \mathbb{N}),\ f(\mathbb{N}, f(\mathbb{N}, \mathbb{N})), ... \}$$

In this way the function nesting can become arbitrary deep. The search space becomes an infinite tree, but the instances stay finite trees.

\paragraph{Expression Equality}
Context free grammars can produce identical strings with a different sequence of production application. The same is true for parameter space definitions.
If a function to measure the equality of two expressions is added to the system, the search space obtains the structure of a directed acyclic graph.
Two expressions can have the same meaning even if the representations are not equal.


%\vspace*{0.5cm}
%\hspace*{-1cm}

\begin{figure}
\includegraphics[scale=0.5]{search_space_hierachy.pdf}

  \caption{The leftmost node represents a system with only primitive spaces. Following the edges from left to write new operations are added. The field of a node hold following information from top to bottom: abbriviation of available operations, structure of search space and structure of resulting parameter. Constants and Variables are not represented.}
  \label{fig_spaces}
\end{figure}



\subsubsection{Further Concepts}



\paragraph{Constants}
So far it is only possible to express search spaces over function trees.
A value that is computed once can only be used in a single other function call. With the introduction of \textit{constants} it is possible to remember the output of computation tree evaluation.
The structure of the describable structure is extended to a DAG and is called a \textit{computation graph}.
Constants allow to express structures beyond the capabilities of a constext free grammar. For example to express a search space over matricies with arbitray size.

\paragraph{Variables}
Variables are like constants, with additional ability to change value in the computation process. The elements of the search space obtain a cylic graph structures.

\subsection{Demands to a good HPO-Algorithm}
A non-exaustive list of general criteria for a good hyperparameter optimization algorithm. ???

\section{Gaussian Processes Regression}
The two big advantages of a Bayesian approach to supervised learning is for one the explicite integration of prior knowledge and for the other a distribution over the output space as an result istead of an point estimate.

\subsection{Kernels}

\subsubsection{Squared-Exponential Kernel}
The squared-exponential kernel (also called radial basis function kernel (RBF)) has many desireble properties. For closer points the correlation tends towards one for while the correlation of farer points tends towards one.

\subsubsection{Kernel Combination}
Kernel combination properties. Maybe: Why do they work!

\section{Bayesian Optimization}
What distinguished \ac{HPO} from other non-\ac{HPO}.

\subsection{Aquisition Functions}

\subsubsection{Probability of Improvement}
\subsubsection{Expected Improvement}

Expected Information (EI) is the most widely used aquistion function. First use was 1998 \cite{jones_efficient_1998}.

$$ \EI(y|x) \coloneqq \int_{-\infty}^{\infty} \max(0, y^*-y)p(y|x)dy $$

For the one-dimensional case a simple closes form formula can be derivated:

$$ \EI(y|x) = \int_{-\infty}^{y^*}(y^*-y)p(y|x)dy$$

$$ = \int_{-\infty}^{y^*}(y^*-y)\normal(y; \mu, \sigma)dy =
\int_{-\infty}^{y^*}(y^*-y)\normal(y-\mu; 0, \sigma)dy$$

$$ = \int_{-\infty}^{y^*}(y^*- t - \mu)\normal(t; 0, \sigma)dt $$

$$ = (y^*-\mu)\int_{-\infty}^{y^*}\normal(t; 0, \sigma)dt - \int_{-\infty}^{y^*}t\cdot\normal(t; 0, \sigma)dt$$

$$ = (y^*-\mu)\Phi(y^*/\sigma) - \phi(y^*/\sigma) $$

Where $\phi$ and $\Phi$ refere to the PDF and CDF of the standart normal distribution.

\subsubsection{Entropy search}
\subsubsection{Upper confidence bound}

\subsection{Aquisition optimization}
The use of a model and an aquisition function leads to a new non-convex optimization problem. Algorithms to find the maximum of the figure of merrit include global optimization technics uschas: DIRECT, L-BFGS-B, ...

\subsubsection{Recommendation Strategy}
The need for a recommendation strategy arises for noisy objective functions. In the deterministic setting the maximal aquisition value becomes the recommendation for the next evaluation step. For non-deterministic aquisition function one needs to generate a sample from a distribution over aquisition values. Strategies like returning the maximum observed value or returning the optimal latent posterior mean can be encountered. Different objective function can require different strategies \cite{hoffman_modular_2014}.

\subsection{SMBO}
Sequential model-based optimization is a very general algorithmic pattern for global optimization with aquistion functions.


\subsection{Conditional Parameters}


\subsubsection{SMAC}


\subsubsection{The Tree-Parzen Algorithm}
The Tree-Parzen Algorithm \cite{bergstra_algorithms_2011} is a Bayesian-Optimization algorithm for tree-structured parameter spaces.

\subsection{Tree and Graph Kernels/Metrics}
...


\section{Bayesain Optimization of Recursive Seach Spaces}

Convential Bayesain optimization algorithms are only suitable for falt multidimesnional search spaces or search spaces containing conditional parameters (SMAC and TreeParzen). The specification language described in the previous chapters howerver allows for search spaces with an arbitrary amount of parameters. The following algorithm allows to employ any Bayesian optimization model for flat parameter spaces to be applied to recursive parameter spaces.

Consider the recursive function tree definition of $G$:

$$ G \leftarrow \operatorname{join()}$$
$$ G \ll (f(G,G)\ |\ A)$$

In the first step Variable $G$ gets initialized with an empty join-node. In the second step a function $f$ with $G$ in both arguments and primitive search space $A$ are inserted into the join-node. The join node has two options, that can be chosen by a selection function, which is similar to rule applicaiton in context free grammars. If we select the first option $f(G,G)$, we are faced with two times two choice options and need to select from four options in the next step.

\begin{figure}

  \begin{dot2tex}[tikz,options=-t math]
    digraph G {

    node [shape=box]

    b [label="f(G,G)"]
    c [label="f(A,A)"]
    d [label="f(A,f(G,G))"]
    e [label="f(f(G,G),A)"]
    f [label="f(f(G,G),f(G,G))"]

    G -> A [label="(0)"]
    G -> b [label="(1)"]
    b -> c [label="(0,0)", lblstyle="left=1.1cm"]
    b -> d [label="(0,1)"]
    b -> e [label="(1,0)"]
    b -> f [label="(1,1)"]

    }
  \end{dot2tex}


  \caption{Three selection levels of search space $G$. }
  \label{levels}
\end{figure}

This selection is continued until the expression containts no more joined nodes.

$$ f(A,A)' \subset f(G,G)' \subset G' $$

\begin{algorithm}[H]
\SetAlgoLined

\end{algorithm}


\subsection{Extention to DAG-structured search spaces}
Identifying equivalent function trees with computer algebra systems.

\subsection{Parallelization}
Simple trick to extend algorithm to parrallel execution.

\subsection{Integration with Manual Search}
Manual guidance before and during the optimization process.

\subsubsection{Prior Information}
In machine learning preknowledge about the learning task can be consindered as choice and distribution over the hypothesis space. This is determined rather indirect by the choice of the \ac{ML}-algorithm and its hyperparameters (or distribution over hyperparameters) or in Bayesian method directly expressed via prior probabilities. Since gaussian processes regession is used here, the choice of the kernel
Prior infotmation can for example be an actual probabilistic prior over the search space. For random sampling this probability can just be used as the sampling probability. For model-based methods, which have no direct sampling, this probability can be interpreted as \textit{probability of improvement} or (more plausible) as \textit{probability of optimum}.

\subsubsection{Search Hints}
A concept I labeled \textit{search hints} is a way to express external knowledge about the search space. I contranst to regular prior knowledge search hits can also be injected into search process on the fly. Intuitively a search hint just denotes good region to search. And once again it is not obvious how to translate this human instructions into the search process.
The easier part is to construct a probability distribution only from hints. If a hint is interpreted as a sample from the space of the probability of optimum then distribution estimation methods like Parzen-Window can be used. If one takes a squard exponential kernel, a hint translates to good region. To indicate better regions one would need to give multiple of samples from the same region, which is unfavourable. To solve this one could give weighted hints. Once again there are different ways to do this.
First one could assign the probability of optimium directly to a region. This however is hard to conceptualize for users and requires not to violate the normalization property (sum of hints must not excede one).
A second way is to allow values between zero and one, fit a curve, and normlize aferwards to get a probability of optimum. Which is much better, because a value of zero keeps the interpretation of zero probability and one get the interpretation of maximum probability. If one allows arbitrary big values for hints the down scaling of former hints could become easier. If one allows also negative values, and includes adding the absolute value of the smalles negative value to all hints, upscaling of former hints would become possible. In both extensions however one looses the interpretation of one and zero.
The third method interpretes hints as a modification of an already existing model.
In an active learning setting point hints can be handeled without an model modification, by simply forcing this hints into the processing queue.

intuitive way to do this is ascribing an increase in probability to positive hints and and decrease in probability to negative hints.

\paragraph{Credebility and Authority}
The \textit{credebility factor} or \textit{authority factor} is a way to ...

\subsection{Objective Function Approximation}


\section{Implementation}
The formalizm has been realized as a package for the Python programming language.

\subsection{Example: Minimize two dimensional function}
The search space construction system has atomic spaces that can
be combined to construct more complex spaces.

\begin{minted}{python}
from treefarm import *

def f:
    return x**2 + 10

# convert function to operation
f = op(f)

# declate parameter space of fucntion f
param = convert([
  R[-10:10],
  R[-10:10],
])

# apply parameter space to function f
search_space = f(param)

result = minimize(f, search_space)

\end{minted}

\subsection{Function Graph Simplification}

\section{Experiments}


\subsection{Gaussian Process Regression}
\subsubsection{Kernel Combination Optimization}
\subsubsection{Kernel Parameter and Kernel Combination Optimization}
\subsection{Elastic Net Linear Regression}
\subsection{Feat-Forward Networks}

\section{Conclusion}

\section{Outlook}
\paragraph{Connection to Functional Programming}
\paragraph{Connection to Constraint Satisfaction Problems}
\paragraph{Connection to Probabilistic Programming}
\paragraph{Connection to Reinforcement Learning}
\paragraph{Integration with Workflow-Managers}

\appendix


\section{Formal descriptions}
Join and prod over parameter-lists form an idempotent semiring:

$(A, \cup)$ is a idempotent commutative monoid (or join-semilatice with zero) with identity element $\emptyset$:
$$(a \cup b) \cup c = a \cup (b \cup c)$$
$$\emptyset \cup a = a \cup \emptyset = a$$
$$a \cup b = b \cup a$$
$$a \cup a = a$$

$(A, \times)$ is a commutative monoid with identity element $[\ ]$:
$$(a \times b) \times c = a \times (b \times c)$$
$$[\ ] \cup a = a \cup [\ ] = a$$

Product left and right distributes over join:
$$a\times(b \cup c) = (a\times b) \cup (a\times c)$$
$$(a \cup b)\times c = (a\times c) \cup (b\times c)$$

Product by $\emptyset$ annihilates $A$:
$$\emptyset \times a = a \times \emptyset = \emptyset$$



\printbibliography


\end{document}
