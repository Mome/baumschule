\documentclass[english]{article}
\usepackage[utf8]{inputenc}
\usepackage{babel}
\usepackage[backend=bibtex]{biblatex}
\usepackage{hyperref}

\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}


\bibliography{master.bib}

\begin{document}  

\title{Bayesian Hyperparameter-Optimization in Grammer-Based Search Spaces}
\author{Moritz Meier}
\maketitle
\tableofcontents
\newpage

\section{Introduction}
\paragraph{Motivation}

\section{Global Black-Box Optimization}
 - Local Optimization
 - Global Optimization
 - Criteria for optimization

\subsection{Bandit Problem}

\section{Hyperparameter Optimization}

\subsection{Machine Learning as Optimization}
 - Types and Error Functions
 - Model Selection

\begin{quote}
Machine learning algorithms, however, have certain characteristics that distinguish them from other
black-box optimization problems.  First, each function evaluation can require a variable amount of
time:  training a small neural network with 10 hidden units will take less time than a bigger net-
work with 1000 hidden units.  Even without considering duration, the advent of cloud computing
makes it possible to quantify economically the cost of requiring large-memory machines for learn-
ing, changing the actual cost in dollars of an experiment with a different number of hidden units.
Second, machine learning experiments are often run in parallel, on multiple cores or machines.  In
both situations, the standard sequential approach of GP optimization can be suboptimal.
\end{quote}
Copied from: \cite{snoek_practical_2012}

\subsection{Algorithm Types}
 - Manual Search
 - Random Search
 - Grid Search
\subsection{Search Space}
\subsubsection{Structure}
 - Categrorical
 - Discrete
 - Continunous
\paragraph{Assumptions}
\subsection{Search Space Construction}
\subsubsection{Search Space Hierachy}


\begin{tabular}{ l | c | c }
Language Feature & Search Space & Parameter Structure \\
\hline
atomic parameters   & one-dim          & single value \\
join                & tree             &              \\
product / operators & multi-dim        & tree         \\
recursion           & infinite tree    &              \\
expression equality & DAG              &              \\
value identity      & ?                & DAG          \\
recursive values    & ?                & ? 
\end{tabular}

\subsection{Demands to a good HPO-Algorithm}

\section{Gaussian Processes Regression}
\subsection{Kernel Combination}


\section{Bayesian Optimization}
\subsection{SMBO}
\subsection{SMAC}
\subsection{The Tree-Parzen Algorithm}
\subsection{Tree and Graph Kernels/Metrics}


\section{Algorithm for Optimization in Grammar-based Search Spaces}
\subsection{Extention to DAG-structured search spaces}
\subsection{Parallelization}
\subsection{Integration with Manual Search}
\subsubsection{Prior Information}
\subsubsection{Search Hints}
\subsection{Objective Function Approximation}


\section{Implementation}
\subsection{Search Space Construction DSL}
Calls are 
\subsection{Algorithm Templates}


\section{Experiments}
\subsection{Gaussian Process Regression}
\subsubsection{Kernel Combination Optimization}
\subsubsection{Kernel Parameter and Kernel Combination Optimization}
\subsection{Elastic Net Linear Regression}
\subsection{Feat-Forward Networks}

\section{Conclusion}

\section{Outlook}
\paragraph{Connection to Functional Programming}
\paragraph{Connection to Constraint Satisfaction Problems}
\paragraph{Connection to Probabilistic Programming}
\paragraph{Connection to Reinforcement Learning}
\paragraph{Integration with Workflow-Managers}
\paragraph{}

\printbibliography

\end{document}  
